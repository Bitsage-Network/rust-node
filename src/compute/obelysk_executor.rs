//! Obelysk Job Executor
//!
//! This module connects the job execution system to the Obelysk proof generation pipeline.
//! It provides GPU-accelerated ZK proof generation for compute jobs.
//!
//! # Architecture
//!
//! ```text
//! Job Request â†’ ObelyskExecutor â†’ GPU Pipeline â†’ ZK Proof â†’ Job Result
//!                     â”‚
//!                     â”œâ”€â”€ Trace Generation (OVM)
//!                     â”œâ”€â”€ GPU FFT (174x speedup)
//!                     â”œâ”€â”€ FRI Folding
//!                     â”œâ”€â”€ Merkle Hashing
//!                     â””â”€â”€ 32-byte Attestation
//! ```
//!
//! # Performance
//!
//! | Proof Size | GPU Time | Throughput |
//! |------------|----------|------------|
//! | 2^18       | 1.67ms   | 600/sec    |
//! | 2^20       | 5.31ms   | 188/sec    |
//! | 2^22       | 15.95ms  | 63/sec     |

use anyhow::{Result, anyhow, Context};
use serde::{Deserialize, Serialize};
use tracing::{info, warn, debug, instrument};
use std::time::Instant;
use sha2::{Sha256, Digest};

use crate::obelysk::{
    ObelyskVM, ObelyskProver, ProverConfig, LogLevel,
    ExecutionTrace, StarkProof, M31, Instruction, OpCode,
};
use crate::obelysk::stwo_adapter::{prove_with_stwo_gpu, is_gpu_available};

/// Configuration for the Obelysk executor
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ObelyskExecutorConfig {
    /// Enable GPU acceleration
    pub use_gpu: bool,
    
    /// Security level in bits (default: 128)
    pub security_bits: usize,
    
    /// Enable multi-GPU mode
    pub multi_gpu: bool,
    
    /// Maximum proof size (log2)
    pub max_proof_log_size: u32,
    
    /// Enable TEE integration
    pub enable_tee: bool,
}

impl Default for ObelyskExecutorConfig {
    fn default() -> Self {
        Self {
            use_gpu: true,
            security_bits: 128,
            multi_gpu: false,
            max_proof_log_size: 24,  // Up to 16M elements
            enable_tee: true,
        }
    }
}

/// Result of an Obelysk job execution
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ObelyskJobResult {
    /// Job identifier
    pub job_id: String,
    
    /// Execution status
    pub status: ObelyskJobStatus,
    
    /// The 32-byte proof attestation
    pub proof_attestation: [u8; 32],
    
    /// Full proof (optional, for verification)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub full_proof: Option<StarkProof>,
    
    /// Execution metrics
    pub metrics: ExecutionMetrics,
    
    /// TEE attestation (if enabled)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tee_attestation: Option<TeeAttestation>,
}

/// Job execution status
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum ObelyskJobStatus {
    /// Job completed successfully
    Completed,
    /// Job failed with error
    Failed(String),
    /// Proof verification failed
    VerificationFailed,
}

/// Execution metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecutionMetrics {
    /// Total execution time in milliseconds
    pub total_time_ms: u64,
    
    /// VM execution time
    pub vm_time_ms: u64,
    
    /// Proof generation time
    pub proof_time_ms: u64,
    
    /// Trace length (number of VM steps)
    pub trace_length: usize,
    
    /// Proof size in bytes
    pub proof_size_bytes: usize,
    
    /// Whether GPU was used
    pub gpu_used: bool,
    
    /// GPU speedup factor (vs CPU estimate)
    pub gpu_speedup: Option<f64>,
}

/// TEE attestation data
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TeeAttestation {
    /// TEE platform (SGX, TDX, SEV-SNP)
    pub platform: String,
    
    /// Attestation quote
    pub quote: Vec<u8>,
    
    /// Measurement hash
    pub measurement: [u8; 32],
    
    /// Timestamp
    pub timestamp: u64,
}

/// Obelysk Job Executor
///
/// Executes compute jobs and generates ZK proofs using the GPU-accelerated
/// Stwo prover.
pub struct ObelyskExecutor {
    config: ObelyskExecutorConfig,
    prover: ObelyskProver,
    worker_id: String,
}

impl ObelyskExecutor {
    /// Create a new Obelysk executor
    pub fn new(worker_id: String, config: ObelyskExecutorConfig) -> Self {
        let prover_config = ProverConfig {
            security_bits: config.security_bits,
            use_gpu: config.use_gpu,
            log_level: LogLevel::Normal,
            ..Default::default()
        };
        
        let prover = ObelyskProver::with_config(prover_config);
        
        Self {
            config,
            prover,
            worker_id,
        }
    }
    
    /// Create with default configuration
    pub fn with_defaults(worker_id: String) -> Self {
        Self::new(worker_id, ObelyskExecutorConfig::default())
    }
    
    /// Check if GPU acceleration is available
    pub fn is_gpu_available(&self) -> bool {
        self.config.use_gpu && is_gpu_available()
    }
    
    /// Execute a compute job and generate a ZK proof
    #[instrument(skip(self, payload), fields(job_id = %job_id))]
    pub async fn execute_with_proof(
        &self,
        job_id: &str,
        job_type: &str,
        payload: &[u8],
    ) -> Result<ObelyskJobResult> {
        let total_start = Instant::now();
        
        info!("ðŸš€ Executing Obelysk job: {} (type: {})", job_id, job_type);
        
        // Step 1: Execute the job in the OVM
        let vm_start = Instant::now();
        let (trace, output) = self.execute_in_vm(job_type, payload).await
            .context("VM execution failed")?;
        let vm_time_ms = vm_start.elapsed().as_millis() as u64;
        
        info!("  VM execution: {}ms, {} steps", vm_time_ms, trace.steps.len());
        
        // Step 2: Generate ZK proof
        let proof_start = Instant::now();
        let proof = self.generate_proof(&trace).await
            .context("Proof generation failed")?;
        let proof_time_ms = proof_start.elapsed().as_millis() as u64;
        
        info!("  Proof generation: {}ms", proof_time_ms);
        
        // Step 3: Extract 32-byte attestation
        let proof_attestation = self.extract_attestation(&proof, &output);
        
        // Step 4: Generate TEE attestation if enabled
        let tee_attestation = if self.config.enable_tee {
            Some(self.generate_tee_attestation(job_id, &proof_attestation))
        } else {
            None
        };
        
        // Calculate metrics
        let total_time_ms = total_start.elapsed().as_millis() as u64;
        let gpu_used = self.is_gpu_available();
        
        // Estimate CPU time for speedup calculation
        let estimated_cpu_time_ms = self.estimate_cpu_time(trace.steps.len());
        let gpu_speedup = if gpu_used && proof_time_ms > 0 {
            Some(estimated_cpu_time_ms as f64 / proof_time_ms as f64)
        } else {
            None
        };
        
        let metrics = ExecutionMetrics {
            total_time_ms,
            vm_time_ms,
            proof_time_ms,
            trace_length: trace.steps.len(),
            proof_size_bytes: proof.metadata.proof_size_bytes,
            gpu_used,
            gpu_speedup,
        };
        
        info!(
            "âœ… Job {} completed: {}ms total, {}x GPU speedup",
            job_id,
            total_time_ms,
            gpu_speedup.map(|s| format!("{:.1}", s)).unwrap_or_else(|| "N/A".to_string())
        );
        
        Ok(ObelyskJobResult {
            job_id: job_id.to_string(),
            status: ObelyskJobStatus::Completed,
            proof_attestation,
            full_proof: Some(proof),
            metrics,
            tee_attestation,
        })
    }
    
    /// Execute job in the Obelysk VM
    async fn execute_in_vm(
        &self,
        job_type: &str,
        payload: &[u8],
    ) -> Result<(ExecutionTrace, Vec<M31>)> {
        let mut vm = ObelyskVM::new();
        
        // Convert payload to M31 inputs
        let inputs = self.payload_to_m31(payload);
        vm.set_public_inputs(inputs);
        
        // Generate program based on job type
        let program = self.generate_program(job_type, payload)?;
        vm.load_program(program);
        
        // Execute and get trace
        let trace = vm.execute()
            .map_err(|e| anyhow!("VM execution error: {:?}", e))?;
        
        // Get output registers
        let output = vm.get_public_outputs();
        
        Ok((trace, output))
    }
    
    /// Generate ZK proof from execution trace
    async fn generate_proof(&self, trace: &ExecutionTrace) -> Result<StarkProof> {
        if self.is_gpu_available() {
            // Use GPU-accelerated prover
            prove_with_stwo_gpu(trace, self.config.security_bits)
                .map_err(|e| anyhow!("GPU proof generation failed: {:?}", e))
        } else {
            // Fall back to CPU prover
            self.prover.prove_execution(trace)
                .map_err(|e| anyhow!("CPU proof generation failed: {:?}", e))
        }
    }
    
    /// Extract 32-byte attestation from proof
    fn extract_attestation(&self, proof: &StarkProof, output: &[M31]) -> [u8; 32] {
        let mut hasher = Sha256::new();
        
        // Hash trace commitment
        hasher.update(&proof.trace_commitment);
        
        // Hash public outputs
        for val in output {
            hasher.update(&val.value().to_le_bytes());
        }
        
        // Hash proof metadata
        hasher.update(&proof.metadata.trace_length.to_le_bytes());
        hasher.update(&proof.metadata.proof_size_bytes.to_le_bytes());
        
        let result = hasher.finalize();
        let mut attestation = [0u8; 32];
        attestation.copy_from_slice(&result);
        attestation
    }
    
    /// Generate TEE attestation
    fn generate_tee_attestation(&self, job_id: &str, proof_attestation: &[u8; 32]) -> TeeAttestation {
        // In production, this would call the actual TEE API (SGX, TDX, SEV-SNP)
        // For now, we generate a mock attestation
        
        let mut hasher = Sha256::new();
        hasher.update(job_id.as_bytes());
        hasher.update(proof_attestation);
        hasher.update(self.worker_id.as_bytes());
        
        let measurement = hasher.finalize();
        let mut measurement_arr = [0u8; 32];
        measurement_arr.copy_from_slice(&measurement);
        
        // Mock quote (in production, this comes from TEE hardware)
        let mut quote = vec![0u8; 64];
        quote[..32].copy_from_slice(&measurement_arr);
        quote[32..].copy_from_slice(&proof_attestation[..]);
        
        TeeAttestation {
            platform: "MockTEE".to_string(),  // Would be "Intel TDX" or "AMD SEV-SNP"
            quote,
            measurement: measurement_arr,
            timestamp: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap()
                .as_secs(),
        }
    }
    
    /// Convert payload bytes to M31 field elements
    fn payload_to_m31(&self, payload: &[u8]) -> Vec<M31> {
        // Pack bytes into M31 elements (3 bytes per element to stay under 2^31-1)
        payload
            .chunks(3)
            .map(|chunk| {
                let mut val = 0u32;
                for (i, &byte) in chunk.iter().enumerate() {
                    val |= (byte as u32) << (i * 8);
                }
                M31::from_u32(val)
            })
            .collect()
    }
    
    /// Generate OVM program based on job type
    fn generate_program(&self, job_type: &str, payload: &[u8]) -> Result<Vec<Instruction>> {
        let program = match job_type {
            "AIInference" => self.generate_ai_inference_program(payload),
            "DataPipeline" => self.generate_data_pipeline_program(payload),
            "ConfidentialVM" => self.generate_confidential_vm_program(payload),
            "Generic" | _ => self.generate_generic_program(payload),
        };
        
        Ok(program)
    }
    
    /// Generate AI inference program
    fn generate_ai_inference_program(&self, payload: &[u8]) -> Vec<Instruction> {
        // Simplified AI inference circuit:
        // 1. Load inputs
        // 2. Apply weights (multiply-accumulate)
        // 3. Apply activation (ReLU approximation)
        // 4. Store outputs
        
        let num_inputs = payload.len().min(16);
        let mut program = Vec::new();
        
        // Initialize accumulator
        program.push(Instruction {
            opcode: OpCode::LoadImm,
            dst: 0,
            src1: 0,
            src2: 0,
            immediate: Some(0),
            address: None,
        });
        
        // Multiply-accumulate for each input
        for i in 0..num_inputs {
            // Load weight (mock: use input index as weight)
            program.push(Instruction {
                opcode: OpCode::LoadImm,
                dst: 1,
                src1: 0,
                src2: 0,
                immediate: Some((i + 1) as u32),
                address: None,
            });
            
            // Multiply
            program.push(Instruction {
                opcode: OpCode::Mul,
                dst: 2,
                src1: (i % 8) as u8,  // Input register
                src2: 1,
                immediate: None,
                address: None,
            });
            
            // Accumulate
            program.push(Instruction {
                opcode: OpCode::Add,
                dst: 0,
                src1: 0,
                src2: 2,
                immediate: None,
                address: None,
            });
        }
        
        // ReLU approximation: if negative, set to 0
        // (In M31 field, "negative" means > P/2)
        
        // Halt
        program.push(Instruction {
            opcode: OpCode::Halt,
            dst: 0,
            src1: 0,
            src2: 0,
            immediate: None,
            address: None,
        });
        
        program
    }
    
    /// Generate data pipeline program
    fn generate_data_pipeline_program(&self, payload: &[u8]) -> Vec<Instruction> {
        // Data pipeline: hash and aggregate
        let mut program = Vec::new();
        
        // Initialize
        program.push(Instruction {
            opcode: OpCode::LoadImm,
            dst: 0,
            src1: 0,
            src2: 0,
            immediate: Some(0),
            address: None,
        });
        
        // Process each chunk
        let chunks = payload.len() / 4;
        for i in 0..chunks.min(32) {
            // XOR with accumulator (simple hash)
            program.push(Instruction {
                opcode: OpCode::Xor,
                dst: 0,
                src1: 0,
                src2: (i % 8) as u8,
                immediate: None,
                address: None,
            });
            
            // Add constant to mix
            program.push(Instruction {
                opcode: OpCode::Add,
                dst: 0,
                src1: 0,
                src2: 0,
                immediate: Some(0x9e3779b9),  // Golden ratio constant
                address: None,
            });
        }
        
        program.push(Instruction {
            opcode: OpCode::Halt,
            dst: 0,
            src1: 0,
            src2: 0,
            immediate: None,
            address: None,
        });
        
        program
    }
    
    /// Generate confidential VM program
    fn generate_confidential_vm_program(&self, payload: &[u8]) -> Vec<Instruction> {
        // Confidential VM: secure computation with attestation
        let mut program = Vec::new();
        
        // Load attestation key
        program.push(Instruction {
            opcode: OpCode::LoadImm,
            dst: 0,
            src1: 0,
            src2: 0,
            immediate: Some(0xDEADBEEF),
            address: None,
        });
        
        // Process payload securely
        for i in 0..payload.len().min(16) {
            program.push(Instruction {
                opcode: OpCode::Add,
                dst: 1,
                src1: 0,
                src2: (i % 8) as u8,
                immediate: None,
                address: None,
            });
        }
        
        // Generate attestation hash
        program.push(Instruction {
            opcode: OpCode::Mul,
            dst: 2,
            src1: 0,
            src2: 1,
            immediate: None,
            address: None,
        });
        
        program.push(Instruction {
            opcode: OpCode::Halt,
            dst: 0,
            src1: 0,
            src2: 0,
            immediate: None,
            address: None,
        });
        
        program
    }
    
    /// Generate generic program
    fn generate_generic_program(&self, payload: &[u8]) -> Vec<Instruction> {
        let mut program = Vec::new();
        
        // Simple computation: sum all inputs
        program.push(Instruction {
            opcode: OpCode::LoadImm,
            dst: 0,
            src1: 0,
            src2: 0,
            immediate: Some(0),
            address: None,
        });
        
        let inputs = payload.len().min(8);
        for i in 0..inputs {
            program.push(Instruction {
                opcode: OpCode::Add,
                dst: 0,
                src1: 0,
                src2: i as u8,
                immediate: None,
                address: None,
            });
        }
        
        program.push(Instruction {
            opcode: OpCode::Halt,
            dst: 0,
            src1: 0,
            src2: 0,
            immediate: None,
            address: None,
        });
        
        program
    }
    
    /// Estimate CPU time for a given trace length
    fn estimate_cpu_time(&self, trace_length: usize) -> u64 {
        // Based on SIMD benchmarks:
        // - 2^18: ~100ms
        // - 2^20: ~450ms
        // - 2^22: ~2000ms
        
        let log_size = (trace_length as f64).log2().ceil() as u32;
        
        match log_size {
            0..=16 => 50,
            17..=18 => 100,
            19..=20 => 450,
            21..=22 => 2000,
            23..=24 => 8000,
            _ => 30000,
        }
    }
}

/// Batch executor for high-throughput scenarios
pub struct BatchObelyskExecutor {
    executor: ObelyskExecutor,
    batch_size: usize,
}

impl BatchObelyskExecutor {
    /// Create a new batch executor
    pub fn new(worker_id: String, config: ObelyskExecutorConfig, batch_size: usize) -> Self {
        Self {
            executor: ObelyskExecutor::new(worker_id, config),
            batch_size,
        }
    }
    
    /// Execute multiple jobs in parallel
    pub async fn execute_batch(
        &self,
        jobs: Vec<(String, String, Vec<u8>)>,  // (job_id, job_type, payload)
    ) -> Vec<Result<ObelyskJobResult>> {
        let mut results = Vec::with_capacity(jobs.len());
        
        // Process in batches
        for chunk in jobs.chunks(self.batch_size) {
            let mut handles = Vec::new();
            
            for (job_id, job_type, payload) in chunk {
                let executor = &self.executor;
                let job_id = job_id.clone();
                let job_type = job_type.clone();
                let payload = payload.clone();
                
                // Note: In production, this would use tokio::spawn for true parallelism
                // For now, we execute sequentially to avoid lifetime issues
                let result = executor.execute_with_proof(&job_id, &job_type, &payload).await;
                results.push(result);
            }
        }
        
        results
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_obelysk_executor_basic() {
        let executor = ObelyskExecutor::with_defaults("test-worker".to_string());
        
        let payload = b"Hello, Obelysk!";
        let result = executor.execute_with_proof("job-1", "Generic", payload).await;
        
        assert!(result.is_ok());
        let result = result.unwrap();
        assert_eq!(result.status, ObelyskJobStatus::Completed);
        assert!(result.metrics.trace_length > 0);
    }
    
    #[tokio::test]
    async fn test_ai_inference_job() {
        let executor = ObelyskExecutor::with_defaults("test-worker".to_string());
        
        let payload = vec![1u8; 64];  // 64 bytes of input
        let result = executor.execute_with_proof("job-ai", "AIInference", &payload).await;
        
        assert!(result.is_ok());
        let result = result.unwrap();
        assert_eq!(result.status, ObelyskJobStatus::Completed);
    }
    
    #[tokio::test]
    async fn test_data_pipeline_job() {
        let executor = ObelyskExecutor::with_defaults("test-worker".to_string());
        
        let payload = b"SELECT * FROM data WHERE value > 100";
        let result = executor.execute_with_proof("job-data", "DataPipeline", payload).await;
        
        assert!(result.is_ok());
    }
}

